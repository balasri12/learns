{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Balajisri\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read the images and generate the train and test dataset\n",
    "# 2. Divide the data set into Train and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4279 images belonging to 12 classes.\n",
      "Found 471 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "image_path = r'D:\\Development\\Plant Seed\\train'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2, validation_split=0.1,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(image_path,\n",
    "                                                 target_size = (128, 128),\n",
    "                                                 batch_size = 32, \n",
    "                                                 subset = 'training',\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = train_datagen.flow_from_directory(image_path,\n",
    "                                            target_size = (128, 128),\n",
    "                                            batch_size = 32,\n",
    "                                            subset = 'validation',\n",
    "                                            class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize & build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(128, (3, 3), input_shape = (128, 128, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    " \n",
    "# Step 4 - Full connection\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dense(128, activation='relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "classifier.add(Dense(units = 12, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the accuracy for both train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "300/300 [==============================] - 114s 379ms/step - loss: 1.8488 - acc: 0.4394 - val_loss: 1.8931 - val_acc: 0.3243\n",
      "Epoch 2/3\n",
      "300/300 [==============================] - 111s 369ms/step - loss: 1.3121 - acc: 0.5786 - val_loss: 2.0693 - val_acc: 0.2394\n",
      "Epoch 3/3\n",
      "300/300 [==============================] - 114s 379ms/step - loss: 0.9505 - acc: 0.6690 - val_loss: 0.9814 - val_acc: 0.6630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2007fce7fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set, steps_per_epoch=300,\n",
    "                         epochs = 3, validation_data=test_set, validation_steps=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have started with initail model with one convolution layer and max pooling with dense which gave me around 70% accuracy. later I have added multiple layer and finally got train accuracy of 93% and validation accuracy of 82%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
